\label{sec:results}

This section summarizes the audited campaign results. Tables~\ref{tab:kpi_A}--\ref{tab:feas_B} report the main-table regime ($N\in\{10,20,40\}$) separately for the baseline and stress time-window families. Table~\ref{tab:scalability_summary} reports the scalability-only regime ($N=80$) without any bound or gap claims.

\subsection{Service KPIs under baseline vs stress time windows}
Family B is designed to stress time-window compliance. The impact is visible in OR-Tools service KPIs at medium size: at $N=20$, the mean on-time rate decreases from 46.3\% (Family A) to 36.3\% (Family B)\evid{C2_A_N20_on_time}\evid{C2_B_N20_on_time}, while mean total tardiness increases from 77.3 to 94.6 minutes\evid{C2_A_N20_tardiness}\evid{C2_B_N20_tardiness}. This aligns with the role of soft time windows: the solver may accept lateness to reduce energy and/or risk penalties.

PyVRP exhibits near-perfect on-time and near-zero tardiness when it finds a feasible solution (Tables~\ref{tab:kpi_A}--\ref{tab:kpi_B}), which is expected under hard time-window enforcement. However, this service-level behavior is coupled with feasibility risk at larger sizes.

\subsection{Feasibility at scale: soft vs hard time-window policies}
A key operational distinction is feasibility under stress. OR-Tools maintains high feasibility at $N=40$ in both families (feasible rate 0.972 in Family A and 0.969 in Family B)\evid{C3_A_N40_feasible_rate_a_n40_ortools}\evid{C3_B_N40_feasible_rate_b_n40_ortools}. In contrast, the hard-TW baseline fails to return feasible solutions at $N=40$ (feasible rate 0)\evid{C3_A_N40_feasible_rate_a_n40_pyvrp}\evid{C3_B_N40_feasible_rate_b_n40_pyvrp}. This illustrates an important managerial tradeoff: hard feasibility constraints can produce attractive service metrics when feasible, but may be brittle under larger instance sizes or stressed time windows.

\subsection{Bound-gap evidence for $N\in\{20,40\}$}
In the bound-gap regime, we compute gaps only when a finite incumbent objective and a compatible finite bound are both available. At $N=20$, OR-Tools yields a tighter mean gap than PyVRP in both families: 11.1\% vs 25.6\% (Family A) and 11.4\% vs 27.8\% (Family B)\evid{C4_A_N20_gap_pct_a_n20_ortools}\evid{C4_A_N20_gap_pct_a_n20_pyvrp}\evid{C4_B_N20_gap_pct_b_n20_ortools}\evid{C4_B_N20_gap_pct_b_n20_pyvrp}. Gaps at $N=40$ are reported where applicable but should be interpreted cautiously due to method-dependent feasibility and the conservative claim regime.

\subsection{Scalability-only characterization at $N=80$}
For $N=80$, we report runtime, feasibility, and KPI summaries only (Table~\ref{tab:scalability_summary}). Per policy, no bound/gap values are computed or claimed at this size. The audit gate confirms that the number of invalid bound/gap rows at $N=80$ is zero\evid{SCAL_n80_invalid_bound_gap_rows}.

\subsection{Sensitivity and risk signal}
Communication risk responds strongly to base-station density and time-window tightness. Table~\ref{tab:managerial_support} and Figure~\ref{fig:bs_delta_effect} show that increasing $B$ reduces mean outage risk and can improve service outcomes. The risk signal is nontrivial across solvers (Table~\ref{tab:risk_signal}), indicating that different route choices can expose materially different communication reliability profiles.

Soft time-window weight also induces an energy--service tradeoff (Figure~\ref{fig:lambda_tw_tradeoff}). This supports the interpretation that penalty calibration is managerial: increasing time-window penalty shifts solutions toward better service compliance at the expense of energy and potentially risk.

\subsection{Statistical reporting (conservative)}
We report adjusted p-values, effect sizes, and bootstrap confidence intervals for paired comparisons. As an example, for runtime (OR-Tools vs PyVRP) the Holm-adjusted p-values are near the 0.05 threshold in both families\evid{C5_A_runtime_p_holm_a}\evid{C5_B_runtime_p_holm_b}, motivating conservative language when interpreting runtime differences.
